{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/670510747/229352-StatisticalLearning/blob/main/Lab10_Image_Segmentation_670510747.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ySbFOKUtu9ej",
      "metadata": {
        "id": "ySbFOKUtu9ej"
      },
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "rattanatham saopha\\\n",
        "670510747"
      ],
      "metadata": {
        "id": "P3lVQPlk8nCs"
      },
      "id": "P3lVQPlk8nCs"
    },
    {
      "cell_type": "markdown",
      "id": "d6TsOLHMpMXG",
      "metadata": {
        "id": "d6TsOLHMpMXG"
      },
      "source": [
        "# Lab 10: Image Segmentation with U-Net\n",
        "\n",
        "In this lab, we move beyond classification (predicting one label per image) to **Semantic Segmentation** (predicting a label for every pixel).\n",
        "\n",
        "We will use the **U-Net architecture**, a powerful encoder-decoder network originally designed for biomedical image segmentation, to segment pets from backgrounds using the **Oxford-IIIT Pet Dataset**.\n",
        "\n",
        "### Learning Objectives:\n",
        "1.  Understand the difference between Classification and Segmentation.\n",
        "2.  Build the building blocks of U-Net (Double Conv, Down, Up).\n",
        "3.  Implement a full U-Net architecture.\n",
        "4.  Evaluate segmentation using Intersection over Union (IoU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LQIpDCDfpMXH",
      "metadata": {
        "id": "LQIpDCDfpMXH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rj7Pnx21pMXH",
      "metadata": {
        "id": "Rj7Pnx21pMXH"
      },
      "source": [
        "## Part 1: Data Loading & Preprocessing\n",
        "\n",
        "We will use the **Oxford-IIIT Pet Dataset**.\n",
        "* **Input:** RGB Image of a pet (cat or dog).\n",
        "* **Target:** A segmentation mask where each pixel denotes the class:\n",
        "    * Class 0: Pet\n",
        "    * Class 1: Background\n",
        "    * Class 2: Border\n",
        "\n",
        "**Note:** To ensure the lab runs in <20 mins, we resize images to `128x128`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F3FMn5UrpMXI",
      "metadata": {
        "id": "F3FMn5UrpMXI"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "class OxfordPetDataset(datasets.OxfordIIITPet):\n",
        "    def __init__(self, root, split, target_types=\"segmentation\", download=False):\n",
        "        super().__init__(root=root, split=split, target_types=target_types, download=download)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, mask = super().__getitem__(idx)\n",
        "\n",
        "        image = TF.resize(image, (IMAGE_SIZE, IMAGE_SIZE), interpolation=transforms.InterpolationMode.BILINEAR)\n",
        "        mask = TF.resize(mask, (IMAGE_SIZE, IMAGE_SIZE), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "\n",
        "        image = TF.to_tensor(image) # Scales [0, 1]\n",
        "        mask = torch.as_tensor(np.array(mask), dtype=torch.long) # Keep integer class labels\n",
        "\n",
        "        mask = mask - 1\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "print(\"Downloading Data... (this may take 1-2 minutes)\")\n",
        "train_data = OxfordPetDataset(root=\"data\", split=\"trainval\", download=True)\n",
        "test_data = OxfordPetDataset(root=\"data\", split=\"test\", download=True)\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=128, shuffle=False)\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_data)}\")\n",
        "print(f\"Testing samples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kNFSRyvtpMXI",
      "metadata": {
        "id": "kNFSRyvtpMXI"
      },
      "outputs": [],
      "source": [
        "def visualize_sample(image, mask, pred=None, title=None):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image.permute(1, 2, 0).cpu()) # C,H,W -> H,W,C\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(mask.cpu(), cmap='jet', vmin=0, vmax=2)\n",
        "    plt.title(\"Ground Truth Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    if pred is not None:\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(pred.cpu(), cmap='jet', vmin=0, vmax=2)\n",
        "        plt.title(\"Predicted Mask\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    if title:\n",
        "        plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "img, mask = train_data[random.randint(0, len(train_data)-1)]\n",
        "visualize_sample(img, mask, title=\"Sample Data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ssV5TMifpMXJ",
      "metadata": {
        "id": "ssV5TMifpMXJ"
      },
      "source": [
        "## Part 2: U-Net Building Blocks ([U-Net paper](https://arxiv.org/pdf/1505.04597))\n",
        "\n",
        "<p align=\"middle\">\n",
        "  <img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"800\" align=\"middle\"/>\n",
        "</p>  \n",
        "\n",
        "U-Net is composed of three main parts:\n",
        "1.  **Encoder:** convolution => [BN] => ReLU + MaxPool.\n",
        "2.  **Decoder:** convolution => [BN] => ReLU + Transpose Convolutions, concatenated with skip connections from the encoder.\n",
        "\n",
        "We define helper classes for these repeating blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "OBeGazNUpMXJ",
      "metadata": {
        "id": "OBeGazNUpMXJ"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"\n",
        "    (Conv -> BatchNorm -> ReLU) * 2\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"\n",
        "    Downscaling with MaxPool -> DoubleConv\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        ]\n",
        "\n",
        "        self.maxpool_conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"\n",
        "    Upscaling -> Concat -> DoubleConv\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # Handle cases where input size is not perfectly divisible by 2\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        # Concatenate along the channel axis\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oLWHnhVLpMXJ",
      "metadata": {
        "id": "oLWHnhVLpMXJ"
      },
      "source": [
        "## Part 3: Demonstration - Small U-Net\n",
        "\n",
        "Here I demonstrate a \"Tiny\" U-Net. It only has 2 levels of depth. It runs very fast but might miss fine details.\n",
        "\n",
        "**Architecture:**\n",
        "* Input -> Conv\n",
        "* Down1 (32 -> 64)\n",
        "* Down2 (64 -> 128)\n",
        "* Up1 (128 -> 64)\n",
        "* Up2 (64 -> 32)\n",
        "* Output -> Classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ImKS15RapMXJ",
      "metadata": {
        "id": "ImKS15RapMXJ"
      },
      "outputs": [],
      "source": [
        "class SmallUNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(SmallUNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Encoder (n_channels => 32 => 64 => 128)\n",
        "        self.conv_in = DoubleConv(3, 32)\n",
        "        self.down1 = Down(32, 64)\n",
        "        self.down2 = Down(64, 128)\n",
        "\n",
        "\n",
        "        # Decoder (128 => 64 => 32 => 1)\n",
        "        self.up1 = Up(128, 64)\n",
        "        self.up2 = Up(64, 32)\n",
        "        self.conv_out = nn.Conv2d(32, 3, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.conv_in(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "\n",
        "        # Decoder (with skip connections)\n",
        "        x = self.up1(x3, x2)\n",
        "        x = self.up2(x, x1)\n",
        "        logits = self.conv_out(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Initialize Model\n",
        "model_small = SmallUNet(n_channels=3, n_classes=3).to(device) # 3 classes: pet, bg, border\n",
        "\n",
        "# Training Function\n",
        "def train_segmentation(model, train_loader, test_loader, epochs=3):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "    print(f\"Training {model.__class__.__name__}...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, masks in test_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f} | Test Loss: {test_loss/len(test_loader):.4f}\")\n",
        "\n",
        "# Train the small model\n",
        "train_segmentation(model_small, train_dataloader, test_dataloader, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_segmentation(model_small, train_dataloader, test_dataloader, epochs=3)"
      ],
      "metadata": {
        "id": "wgdG2E0Lco4O"
      },
      "id": "wgdG2E0Lco4O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "i2YrMSacubnY",
      "metadata": {
        "id": "i2YrMSacubnY"
      },
      "source": [
        "Visualize results from UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qeD095p-pMXK",
      "metadata": {
        "id": "qeD095p-pMXK"
      },
      "outputs": [],
      "source": [
        "model_small.eval()\n",
        "img, mask = test_data[0]\n",
        "with torch.no_grad():\n",
        "    pred = model_small(img.unsqueeze(0).to(device))\n",
        "    pred_mask = torch.argmax(pred, dim=1).squeeze(0)\n",
        "\n",
        "visualize_sample(img, mask, pred_mask, title=\"Small U-Net Prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mOrq_7EqpMXL",
      "metadata": {
        "id": "mOrq_7EqpMXL"
      },
      "source": [
        "### Evaluation: Intersection over Union (IoU)\n",
        "\n",
        "Accuracy is misleading in segmentation because background pixels dominate the image. The standard metric is **IoU (Intersection over Union)**, also known as the Jaccard Index.\n",
        "\n",
        "$$ IoU = \\frac{\\text{Intersection}}{\\text{Union}} = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{TP}{TP + FP + FN} $$\n",
        "\n",
        "Where:\n",
        "* $A$ is the Ground Truth mask.\n",
        "* $B$ is the Predicted mask.\n",
        "* $TP$ = True Positive (Pixel correctly predicted as class X)\n",
        "* $FP$ = False Positive (Pixel incorrectly predicted as class X)\n",
        "* $FN$ = False Negative (Pixel of class X missed by prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xqae0_RjpMXL",
      "metadata": {
        "id": "xqae0_RjpMXL"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(pred_mask, true_mask, num_classes=3):\n",
        "    iou_list = []\n",
        "    pred_mask = pred_mask.view(-1)\n",
        "    true_mask = true_mask.view(-1)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = pred_mask == cls\n",
        "        target_inds = true_mask == cls\n",
        "        intersection = (pred_inds & target_inds).sum().item()\n",
        "        union = (pred_inds | target_inds).sum().item()\n",
        "\n",
        "        if union == 0:\n",
        "            iou_list.append(float('nan')) # Ignore if class not present\n",
        "        else:\n",
        "            iou_list.append(intersection / union)\n",
        "    return np.nanmean(iou_list)\n",
        "\n",
        "# Calculate IoU for your model (uncomment below after implementing UNet)\n",
        "model_small.eval()\n",
        "total_iou = 0\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_dataloader:\n",
        "        images = images.to(device)\n",
        "        outputs = model_small(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        total_iou += calculate_iou(preds.cpu(), masks)\n",
        "        count += 1\n",
        "print(f\"Mean IoU on Test Set: {total_iou/count:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iDhOOHU_pMXK",
      "metadata": {
        "id": "iDhOOHU_pMXK"
      },
      "source": [
        "## Part 4: Exercise - Implement a Deeper U-Net\n",
        "\n",
        "The Small U-Net works, but a standard U-Net is deeper. Deeper networks capture more complex features.\n",
        "\n",
        "**Your Task:**\n",
        "1.  Implement a bigger `UNet` model below. I recommend for the encoder to go a lot higher than 128 channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dXc7Ht-9pMXK",
      "metadata": {
        "id": "dXc7Ht-9pMXK"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # --- Write your code here ---\n",
        "        # 1. Define Encoder\n",
        "        def double_conv(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        # 1. Define Encoder\n",
        "        self.enc1 = double_conv(n_channels, 64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc2 = double_conv(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc3 = double_conv(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc4 = double_conv(256, 512)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck (going all the way up to 1024 channels)\n",
        "        self.bottleneck = double_conv(512, 1024)\n",
        "\n",
        "        # 2. Define Decoder\n",
        "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dec4 = double_conv(1024, 512) # 512 from up4 + 512 from enc4\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = double_conv(512, 256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = double_conv(256, 128)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = double_conv(128, 64)\n",
        "\n",
        "        # Output classification layer\n",
        "        self.out = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Forward pass through Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        e3 = self.enc3(self.pool2(e2))\n",
        "        e4 = self.enc4(self.pool3(e3))\n",
        "\n",
        "        b = self.bottleneck(self.pool4(e4))\n",
        "\n",
        "        # 2. Forward pass through Decoder with Skip Connections\n",
        "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
        "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
        "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
        "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
        "\n",
        "        return self.out(d1)\n",
        "\n",
        "# Instantiate your model\n",
        "# student_model = UNet(n_channels=3, n_classes=3).to(device)\n",
        "\n",
        "# Train your model\n",
        "# train_segmentation(student_model, train_dataloader, test_dataloader, epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Train the model."
      ],
      "metadata": {
        "id": "k-0HysGC0QuP"
      },
      "id": "k-0HysGC0QuP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate your model\n",
        "student_model = UNet(n_channels=3, n_classes=3).to(device)\n",
        "\n",
        "# Train your model\n",
        "train_segmentation(student_model, train_dataloader, test_dataloader, epochs=15)"
      ],
      "metadata": {
        "id": "FkJqE3kj0X31"
      },
      "id": "FkJqE3kj0X31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Choose one test image whose model's segmentation is reasonably accurate. Visualize the model's segmentation of the chosen image."
      ],
      "metadata": {
        "id": "Wus5Pq8l0TS3"
      },
      "id": "Wus5Pq8l0TS3"
    },
    {
      "cell_type": "code",
      "source": [
        "student_model.eval()\n",
        "with torch.no_grad():\n",
        "    # Grab a batch from the test dataset\n",
        "    images, masks = next(iter(test_dataloader))\n",
        "    images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "    # Generate predictions\n",
        "    outputs = student_model(images)\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    # Choose a reasonably accurate image from the batch (e.g., index 0)\n",
        "    idx = 0\n",
        "    img_display = images[idx].cpu().permute(1, 2, 0).numpy()\n",
        "    mask_display = masks[idx].cpu().numpy()\n",
        "    pred_display = preds[idx].cpu().numpy()\n",
        "\n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes[0].imshow(img_display)\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(mask_display, vmin=0, vmax=2)\n",
        "    axes[1].set_title('True Mask')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(pred_display, vmin=0, vmax=2)\n",
        "    axes[2].set_title('Predicted Mask')\n",
        "    axes[2].axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1W_2z4X40nkN"
      },
      "id": "1W_2z4X40nkN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Calculate the mean IoU of the model's segmentations on the test set."
      ],
      "metadata": {
        "id": "zLF9is0Z0VHG"
      },
      "id": "zLF9is0Z0VHG"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_iou(preds, labels, num_classes):\n",
        "    ious = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = preds == cls\n",
        "        target_inds = labels == cls\n",
        "        intersection = (pred_inds & target_inds).sum().item()\n",
        "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
        "\n",
        "        if union > 0:\n",
        "            ious.append(intersection / union)\n",
        "        else:\n",
        "            ious.append(float('nan'))\n",
        "    return np.nanmean(ious)\n",
        "\n",
        "student_model.eval()\n",
        "total_iou = 0\n",
        "num_batches = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in tqdm(test_dataloader, desc=\"Calculating IoU\"):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = student_model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        batch_iou = calculate_iou(preds, masks, num_classes=3)\n",
        "        if not np.isnan(batch_iou):\n",
        "            total_iou += batch_iou\n",
        "            num_batches += 1\n",
        "\n",
        "mean_iou = total_iou / num_batches\n",
        "print(f\"Mean IoU on the test set: {mean_iou:.4f}\")"
      ],
      "metadata": {
        "id": "GHOHBvwr2doA"
      },
      "id": "GHOHBvwr2doA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean IoU on the test set: 0.6856"
      ],
      "metadata": {
        "id": "H0yA8NNK9bqP"
      },
      "id": "H0yA8NNK9bqP"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}